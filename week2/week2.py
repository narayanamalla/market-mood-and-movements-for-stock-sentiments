# -*- coding: utf-8 -*-
"""week2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NOY3UUBSZlie4W6vBx5fCpQHVF92yd3T
"""

"""
Demonstration of why static embeddings fail and BERT succeeds
"""

from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

sentences = [
    "He deposited money in the bank",
    "The fisherman sat on the bank of the river"
]

inputs = tokenizer(sentences, return_tensors="pt", padding=True)
outputs = model(**inputs)

# Extract embeddings for the word 'bank'
bank_token_id = tokenizer.convert_tokens_to_ids("bank")
embeddings = outputs.last_hidden_state

bank_embeddings = []
for i, sentence in enumerate(inputs["input_ids"]):
    idx = (sentence == bank_token_id).nonzero(as_tuple=True)[0]
    bank_embeddings.append(embeddings[i, idx].mean(dim=0))

cos_sim = torch.nn.functional.cosine_similarity(
    bank_embeddings[0], bank_embeddings[1], dim=0
)

print("Cosine similarity between 'bank' embeddings:", cos_sim.item())

from transformers.models.bert.modeling_bert import BertEmbeddings
from transformers import BertConfig
import torch

# Load BERT configuration
config = BertConfig()

# Initialize embedding layer
bert_embeddings = BertEmbeddings(config)

# Example input: [CLS] this is a test [SEP]
input_ids = torch.tensor([[101, 2023, 2003, 1037, 3231, 102]])
token_type_ids = torch.zeros_like(input_ids)

# Forward pass
embeddings = bert_embeddings(
    input_ids=input_ids,
    token_type_ids=token_type_ids
)

print("Embedding tensor shape:", embeddings.shape)

import random

def apply_bert_masking(tokens, tokenizer):
    masked_tokens = tokens.copy()
    labels = [-100] * len(tokens)

    for i in range(len(tokens)):
        if random.random() < 0.15:
            labels[i] = tokens[i]
            prob = random.random()

            if prob < 0.8:
                masked_tokens[i] = tokenizer.mask_token_id
            elif prob < 0.9:
                masked_tokens[i] = random.randint(0, tokenizer.vocab_size - 1)
            else:
                pass  # keep original

    return masked_tokens, labels

"""
Stage 1: Base BERT (General English)
Stage 2: Domain-Adaptive Pretraining (TRC2 Financial News)
Stage 3: Task-Specific Fine-Tuning (Financial PhraseBank)
"""

class FinBERTPipeline:
    def __init__(self):
        self.stage1 = "bert-base-uncased"
        self.stage2 = "Domain Adaptive Pretraining"
        self.stage3 = "Sentiment Fine-tuning"

    def describe(self):
        return {
            "Stage 1": self.stage1,
            "Stage 2": "Pretrain on Financial News (MLM)",
            "Stage 3": "Fine-tune on labeled sentiment data"
        }

pipeline = FinBERTPipeline()
pipeline.describe()

"""
Domain Adaptation via continued MLM pretraining
"""

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./finbert-domain-adapt",
    overwrite_output_dir=True,
    per_device_train_batch_size=16,
    num_train_epochs=2,
    save_steps=10_000,
    save_total_limit=2,
)

# Trainer would be initialized with TRC2 dataset

dataset_usage = {
    "TRC2_Financial": "Large unlabeled corpus → language understanding",
    "Financial_PhraseBank": "Small labeled dataset → sentiment classification"
}

dataset_usage

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F

MODEL_NAME = "ProsusAI/finbert"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

model.eval()

import torch
import numpy as np

MAX_LEN = 512

def chunk_text(text, tokenizer, max_len=512):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    return [tokens[i:i+max_len] for i in range(0, len(tokens), max_len)]

def predict_sentiment_long_doc(text):
    chunks = chunk_text(text, tokenizer)
    probs_list = []

    for chunk in chunks:
        input_ids = torch.tensor([[tokenizer.cls_token_id] + chunk + [tokenizer.sep_token_id]])
        attention_mask = torch.ones_like(input_ids)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            probs = F.softmax(outputs.logits, dim=-1)

        probs_list.append(probs.numpy())

    avg_probs = np.mean(probs_list, axis=0)
    sentiment = ["negative", "neutral", "positive"][np.argmax(avg_probs)]

    return {
        "sentiment": sentiment,
        "confidence": avg_probs.tolist()
    }

news = """
The company reported strong quarterly earnings beating market expectations.
However, rising inflation and interest rate uncertainty remain concerns.
"""

predict_sentiment_long_doc(news)

#let's check for tesla

!pip install transformers torch matplotlib numpy

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

MODEL_NAME = "ProsusAI/finbert"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

model.eval()

tesla_news = """
New Street Research raised its price target to a street-high $600 (up from $520),
suggesting nearly 38.6% upside potential from recent levels.

This rally follows a broader surge in 2024, where Tesla’s stock rose roughly 71%,
with most gains occurring after Donald Trump’s election.

Tesla reported Q4 vehicle deliveries of 418,000, which fell short of expectations,
leading HSBC to maintain a 'Reduce' rating on the stock.
"""

MAX_LEN = 512

def chunk_text(text, tokenizer, max_len=512):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    return [tokens[i:i+max_len] for i in range(0, len(tokens), max_len)]

def predict_sentiment_long_doc(text):
    chunks = chunk_text(text, tokenizer)
    all_probs = []

    for chunk in chunks:
        input_ids = torch.tensor(
            [[tokenizer.cls_token_id] + chunk + [tokenizer.sep_token_id]]
        )
        attention_mask = torch.ones_like(input_ids)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            probs = F.softmax(outputs.logits, dim=-1)

        all_probs.append(probs.numpy()[0])

    avg_probs = np.mean(all_probs, axis=0)
    labels = ["Negative", "Neutral", "Positive"]

    return labels, avg_probs

labels, probs = predict_sentiment_long_doc(tesla_news)
labels, probs
plt.figure()
plt.bar(labels, probs)
plt.title("Tesla Stock News Sentiment (FinBERT)")
plt.xlabel("Sentiment")
plt.ylabel("Probability")
plt.ylim(0, 1)
plt.show()